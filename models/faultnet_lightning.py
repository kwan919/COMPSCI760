# -*- coding: utf-8 -*-
"""FaultNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOXzwRqadeg2NYKSRH5_7ch-zji28jRH
"""

GENERATE_DATA = False

# ! pip install --quiet "pandas" "ipython[notebook]" "torchvision" "setuptools==59.5.0" "torch>=1.8" "torchmetrics>=0.7" "seaborn" "pytorch-lightning>=1.4"

import os

import pandas as pd
import torch
from IPython.core.display import display
from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks.progress import TQDMProgressBar
from pytorch_lightning.loggers import CSVLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchmetrics import Accuracy
from torchvision import transforms
from torchvision.datasets import MNIST
from sklearn import preprocessing

# from google.colab import drive
# drive.mount('/content/drive')

"""## Generate Data Matrices"""

import numpy as np

from pandas import DataFrame
from tqdm import tqdm

if GENERATE_DATA:
  class Restructure():
      def __init__(self, csv_data: DataFrame) -> None:
          self.csv_data = csv_data

      def to_windows(self, m: int, ) -> DataFrame:
          """Transform the data from raw to the windowed data
              row size = m x number_of_features, 
                  The last data could not form a window will be dropped

          Args:
              m (int): number of features in windows

          Returns:
              sig: np matrix 
          """
          total_length = len(self.csv_data)
          number_of_windows = int(total_length / m)
          columns_name = self.csv_data.columns
          columns_length = len(columns_name)

          data_array = self.csv_data.to_numpy()[:number_of_windows*m, :]
          new_data_array = []
          for row_matrix in tqdm(np.array_split(data_array, number_of_windows)):
              reshape_array = np.empty((columns_length, int(np.sqrt(m)), int(np.sqrt(m))))
              for col in range(columns_length): 
                  reshape_signal = row_matrix[:, col].reshape(int(np.sqrt(m)), int(np.sqrt(m)))
                  reshape_array[col, :, :] = reshape_signal
              new_data_array.append(reshape_array)
          new_data_array = np.stack(new_data_array, axis=0)

          return new_data_array

if GENERATE_DATA: 
  healthy_name_list = ["K001", "K002", "K003", "K004", "K005", "K006"]
  healthy_df_list = []
  for k_name in healthy_name_list:
      for i in range(4):
          df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" + str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
          del df["temp"]
          print("Start convert", k_name, "- K", i)
          healthy_df_list.append(Restructure(df).to_windows(1600))
  healthy_df = np.concatenate(healthy_df_list)
  healthy_label = np.repeat(0, healthy_df.shape[0])

  ir_name_list = ["KA01", "KA03", "KA05", "KA06", "KA07", "KA08", "KA09", "KA04", "KA15", "KA16", "KA22", "KA30"]
  ir_df_list = []
  for k_name in ir_name_list:
      for i in range(4):
          df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" + str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
          del df["temp"]
          print("Start convert", k_name, "- K", i)
          ir_df_list.append(Restructure(df).to_windows(1600))
  ir_df = np.concatenate(ir_df_list)
  ir_label = np.repeat(1, ir_df.shape[0])

  or_name_list = ["KI01", "KI03", "KI04", "KI05", "KI07", "KI08", "KI14", "KI16", "KI17", "KI18", "KI21"]
  # "KB23", "KB24", "KB27" are IR + OR
  or_df_list = []
  for k_name in or_name_list:
      for i in range(4):
          df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" + str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
          del df["temp"]
          print("Start convert", k_name, "- K", i)
          or_df_list.append(Restructure(df).to_windows(1600))
  or_df = np.concatenate(or_df_list)
  or_label = np.repeat(2, or_df.shape[0])

  X = np.concatenate((healthy_df, ir_df, or_df))
  y = np.concatenate((healthy_label, ir_label, or_label))

"""## Split data and create helper classes/functions"""

class AddGaussianNoise(object):
    def __init__(self, mean=0., snr=-4., clip=None, rayleigh=False, method='torch', channel=None):
        self.method = method
        self.rayleigh = rayleigh
        self.clip = clip
        self.snr = snr
        self.mean = mean
        self.channel = channel
        self.std = 1/(10**(self.snr/10))

    def __call__(self, tensor):
        if self.method == 'numpy':
          if self.clip != None: 
            noise = torch.from_numpy(np.clip(np.random.normal(0, 1, (40, 40)), -self.clip, self.clip))
          elif self.rayleigh and self.channel == None:
            m = 10**(self.snr/20) 
            r = np.random.rayleigh(size=(40, 40))
            norm_r = preprocessing.StandardScaler().fit_transform(r)
            noise = torch.from_numpy(norm_r/m)
          elif self.rayleigh and self.channel != None:
            m = 10**(self.snr/20) 
            r = np.random.rayleigh(size=(40, 40))
            norm_r = preprocessing.StandardScaler().fit_transform(r)
            noise = torch.from_numpy(norm_r/m)
            tensor[self.channel, :, :] = tensor[self.channel, :, :] + noise
            return tensor
          else:
            noise = torch.from_numpy(np.random.normal(0, self.std, (40, 40)))
          return tensor + noise
        else: 
          if self.channel != None:
            torch.manual_seed(0)
            tensor[self.channel, :, :] = tensor[self.channel, :, :] + (torch.randn(tensor[self.channel, :, :].size()) * self.std + self.mean)
            return tensor 
          else: 
            torch.manual_seed(0)
            return tensor + torch.randn(tensor.size()) * self.std + self.mean
    
    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)

from torch.utils.data import Dataset, TensorDataset
import random

class SignalDataset(Dataset):
    """TensorDataset with support of transforms.
    """
    def __init__(self, tensors, transform=None):
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)
        self.tensors = tensors
        self.transform = transform

    def __getitem__(self, index):
        x = self.tensors[0][index]

        random.seed(0) 
        np.random.seed(0)
        torch.manual_seed(0)
        if self.transform:
            x = self.transform(x)

        y = self.tensors[1][index]

        return x, y

    def __len__(self):
        return self.tensors[0].size(0)

import torchvision.transforms as transforms

def get_mean_and_std(dataloader):
    channels_sum, channels_squared_sum, num_batches = 0, 0, 0
    for data, _ in dataloader:
        # Mean over batch, height and width, but not over the channels
        channels_sum += torch.mean(data, dim=[0,2,3])
        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])
        num_batches += 1
    
    mean = channels_sum / num_batches

    # std = sqrt(E[X^2] - (E[X])^2)
    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5

    return mean, std

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(6, 32, kernel_size=4,stride=1,padding = 1)
        self.bn1 = nn.BatchNorm2d(32)
        self.mp1 = nn.MaxPool2d(kernel_size=4,stride=2)
        self.conv2 = nn.Conv2d(32,64, kernel_size=4,stride =1)
        self.bn2 = nn.BatchNorm2d(64)
        self.mp2 = nn.MaxPool2d(kernel_size=4,stride=2)
        self.fc1= nn.Linear(2304,256)
        self.bn3 = nn.BatchNorm2d(256)
        self.dp1 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(256,3)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp1(self.conv1(x)))
        x = F.relu(self.mp2(self.conv2(x)))
        x = x.view(in_size,-1)
        x = F.relu(self.fc1(x))
        x = self.dp1(x)
        x = self.fc2(x)
        
        return x

class FaultDetect(LightningModule):
    def __init__(self, add_noise=False, model=CNN().double(), learning_rate=1e-3):
        super().__init__()

        # Set our init args as class attributes
        self.add_noise = add_noise
        self.learning_rate = learning_rate
        self.train_mean = None
        self.train_std = None
        self.train_mean2 = None
        self.train_std2 = None
        self.train_tf = None
        self.test_tf = None

        # Define PyTorch model
        self.model = model
        self.val_accuracy = Accuracy()
        self.test_accuracy = Accuracy()

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x.double())
        loss = F.cross_entropy(logits, y.long())

        # Calling self.log will surface up scalars for you in TensorBoard
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x.double())
        loss = F.cross_entropy(logits, y.long())
        _, preds = torch.max(F.log_softmax(logits, dim=1), 1)
        self.val_accuracy(preds, y)

        # Calling self.log will surface up scalars for you in TensorBoard
        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", self.val_accuracy, prog_bar=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x.double())
        loss = F.cross_entropy(logits, y.long())
        _, preds = torch.max(F.log_softmax(logits, dim=1), 1)
        self.test_accuracy.update(preds, y)

        # Calling self.log will surface up scalars for you in TensorBoard
        self.log("test_loss", loss, prog_bar=True)
        self.log("test_acc", self.test_accuracy, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        return optimizer

    ####################
    # DATA RELATED HOOKS
    ####################

    def setup(self, stage=None):
        X = np.load('X.npy')
        y = np.load('y.npy')

        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # np.save('X_test', X_test)
        # np.save('y_test', y_test)

        # Assign train/val datasets for use in dataloaders
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
        self.train_set = SignalDataset((torch.from_numpy(X_train), torch.from_numpy(y_train)), transform=self.train_tf)
        train_loader = DataLoader(dataset = self.train_set, batch_size = 128, shuffle = True)
        self.train_mean, self.train_std = get_mean_and_std(train_loader)
        if self.add_noise:
          self.train_tf = transforms.Compose(
              [   
                  transforms.Normalize(tuple(self.train_mean.tolist()), tuple(self.train_std.tolist())),
                  transforms.RandomApply([AddGaussianNoise(mean=0, snr=8, method='numpy', rayleigh=True, channel=None)], 0.2),
              ]
          ) 
          self.train_set = SignalDataset((torch.from_numpy(X_train), torch.from_numpy(y_train)), transform=self.train_tf)
          train_loader = DataLoader(dataset = self.train_set, batch_size = 128, shuffle = True)
          self.train_mean2, self.train_std2 = get_mean_and_std(train_loader)
          self.train_tf = transforms.Compose(
              [   
                  transforms.Normalize(tuple(self.train_mean.tolist()), tuple(self.train_std.tolist())),
                  transforms.RandomApply([AddGaussianNoise(mean=0, snr=8, method='numpy', rayleigh=True, channel=None)], 0.2),
                  transforms.Normalize(tuple(self.train_mean2.tolist()), tuple(self.train_std2.tolist()))
              ]
          ) 
        else: 
          self.train_tf = transforms.Compose(
              [               
                  transforms.Normalize(tuple(self.train_mean.tolist()), tuple(self.train_std.tolist()))
              ]
          )
        self.train_set = SignalDataset((torch.from_numpy(X_train), torch.from_numpy(y_train)), transform=self.train_tf)
        self.val_set = SignalDataset((torch.from_numpy(X_val), torch.from_numpy(y_val)), transform=self.train_tf)

    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=128, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=128, shuffle=False)

"""## Define model and train"""

model = FaultDetect(
    model=CNN().double(),
    add_noise=True,
    learning_rate=1e-3
)

trainer = Trainer(
    accelerator="auto",
    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs
    gpus=[2],
    max_epochs=100,
    callbacks=[TQDMProgressBar(refresh_rate=20), ModelCheckpoint(monitor="val_loss")],
    logger=CSVLogger(save_dir="logs/")
)

TRAIN = False
EVALUATE = True

if TRAIN:
    trainer.fit(model)

"""## Evaluation"""

if EVALUATE:
    X_test = np.load('X_test.npy')
    y_test = np.load('y_test.npy')
    # ckpt_path = trainer.checkpoint_callback.best_model_path
    ckpt_path = 'logs/default/rayleigh_old/checkpoints/epoch=41-step=4871.ckpt'

    model = FaultDetect.load_from_checkpoint(ckpt_path)
    # model.setup()
    snr = [-4, 0, 4, 8]    
    channel = [None, 0, 1, 2, 3, 4, 5]

    for s in snr: 
        for c in channel:
            print('SETTINGS: '+str(s)+' AND '+str(c))
            test_set = SignalDataset((torch.from_numpy(X_test), torch.from_numpy(y_test)), transform=None)

            test_loader = DataLoader(dataset = test_set, batch_size = 1024, shuffle = False)
            test_mean, test_std = get_mean_and_std(test_loader)

            test_tf = transforms.Compose(
                [   
                    transforms.Normalize(tuple(test_mean.tolist()), tuple(test_std.tolist())),
                    AddGaussianNoise(mean=0, snr=s, method='numpy', rayleigh=True, channel=c),
                ]
            )

            test_set = SignalDataset((torch.from_numpy(X_test), torch.from_numpy(y_test)), transform=test_tf)
            test_loader = DataLoader(dataset = test_set, batch_size = 128, shuffle = False)

            # test_loader = DataLoader(dataset = test_set, batch_size = 128, shuffle = True)
            test_mean2, test_std2 = get_mean_and_std(test_loader)

            test_tf = transforms.Compose(
                [   
                    transforms.Normalize(tuple(test_mean.tolist()), tuple(test_std.tolist())),
                    AddGaussianNoise(mean=0, snr=s, method='numpy', rayleigh=True, channel=c),
                    transforms.Normalize(tuple(test_mean2.tolist()), tuple(test_std2.tolist())),
                ]
            )

            test_set = SignalDataset((torch.from_numpy(X_test), torch.from_numpy(y_test)), transform=test_tf)
            test_loader = DataLoader(dataset = test_set, batch_size = 1024, shuffle = False)

            trainer.test(model, test_loader)

