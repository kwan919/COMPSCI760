# -*- coding: utf-8 -*-
"""FaultNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOXzwRqadeg2NYKSRH5_7ch-zji28jRH
"""

import torch.utils.data as data_utils
import torchvision.transforms as transforms
from torch.utils.data import Dataset, TensorDataset
from tqdm import tqdm
from pandas import DataFrame
import numpy as np
import pandas as pd
import sys
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

import torch
import torch.nn as nn
from torchvision.datasets import CIFAR10
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
from torch.optim import Adam
from torch.autograd import Variable
import torch.nn.functional as F

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   
os.environ["CUDA_VISIBLE_DEVICES"]="4,5,6,7,8,9"  # specify which GPU(s) to be used

GENERATE_DATA = False
TRAIN_MODE = False
INFERENCE_MODE = True

if GENERATE_DATA:

    class Restructure():
        def __init__(self, csv_data: DataFrame) -> None:
            self.csv_data = csv_data

        def to_windows(self, m: int, ) -> DataFrame:
            """Transform the data from raw to the windowed data
                row size = m x number_of_features, 
                    The last data could not form a window will be dropped

            Args:
                m (int): number of features in windows

            Returns:
                sig: np matrix 
            """
            total_length = len(self.csv_data)
            number_of_windows = int(total_length / m)
            columns_name = self.csv_data.columns
            columns_length = len(columns_name)

            data_array = self.csv_data.to_numpy()[:number_of_windows*m, :]
            new_data_array = []
            for row_matrix in tqdm(np.array_split(data_array, number_of_windows)):
                reshape_array = np.empty(
                    (columns_length, int(np.sqrt(m)), int(np.sqrt(m))))
                for col in range(columns_length):
                    reshape_signal = row_matrix[:, col].reshape(
                        int(np.sqrt(m)), int(np.sqrt(m)))
                    reshape_array[col, :, :] = reshape_signal
                new_data_array.append(reshape_array)
            new_data_array = np.stack(new_data_array, axis=0)

            return new_data_array

"""## Generate Data Matrices"""

if GENERATE_DATA:
    healthy_name_list = ["K001", "K002", "K003", "K004", "K005", "K006"]
    healthy_df_list = []
    for k_name in healthy_name_list:
        for i in range(4):
            df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" +
                             str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
            del df["temp"]
            print("Start convert", k_name, "- K", i)
            healthy_df_list.append(Restructure(df).to_windows(1600))
    healthy_df = np.concatenate(healthy_df_list)
    healthy_label = np.repeat(0, healthy_df.shape[0])

    ir_name_list = ["KA01", "KA03", "KA05", "KA06", "KA07",
                    "KA08", "KA09", "KA04", "KA15", "KA16", "KA22", "KA30"]
    ir_df_list = []
    for k_name in ir_name_list:
        for i in range(4):
            df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" +
                             str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
            del df["temp"]
            print("Start convert", k_name, "- K", i)
            ir_df_list.append(Restructure(df).to_windows(1600))
    ir_df = np.concatenate(ir_df_list)
    ir_label = np.repeat(1, ir_df.shape[0])

    or_name_list = ["KI01", "KI03", "KI04", "KI05", "KI07",
                    "KI08", "KI14", "KI16", "KI17", "KI18", "KI21"]
    # "KB23", "KB24", "KB27" are IR + OR
    or_df_list = []
    for k_name in or_name_list:
        for i in range(4):
            df = pd.read_csv("drive/MyDrive/down_sampled/" + k_name + "_K" +
                             str(i) + "_down.csv", engine='c', low_memory=False, memory_map=True)
            del df["temp"]
            print("Start convert", k_name, "- K", i)
            or_df_list.append(Restructure(df).to_windows(1600))
    or_df = np.concatenate(or_df_list)
    or_label = np.repeat(2, or_df.shape[0])

    X = np.concatenate((healthy_df, ir_df, or_df))
    y = np.concatenate((healthy_label, ir_label, or_label))


"""## Split data and create torch datasets"""
X = np.load('X.npy')
y = np.load('y.npy')

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)


class SignalDataset(Dataset):
    """TensorDataset with support of transforms.
    """

    def __init__(self, tensors, transform=None):
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)
        self.tensors = tensors
        self.transform = transform

    def __getitem__(self, index):
        x = self.tensors[0][index]

        if self.transform:
            x = self.transform(x)

        y = self.tensors[1][index]

        return x, y

    def __len__(self):
        return self.tensors[0].size(0)


# class AddGaussianNoise(object):
#     def __init__(self, mean=0., snr=1.):
#         self.snr = snr
#         self.mean = mean
#         self.std = 1/(10**(snr/10))

#     def __call__(self, tensor):
#         torch.manual_seed(0)
#         return tensor + torch.randn(tensor.size()) * self.std + self.mean

#     def __repr__(self):
#         return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)

X_train = torch.from_numpy(X_train)
X_test = torch.from_numpy(X_test)
y_train = torch.from_numpy(y_train)
y_test = torch.from_numpy(y_test)


def get_mean_and_std(dataloader):
    channels_sum, channels_squared_sum, num_batches = 0, 0, 0
    for data, _ in dataloader:
        # Mean over batch, height and width, but not over the channels
        channels_sum += torch.mean(data, dim=[0, 2, 3])
        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])
        num_batches += 1

    mean = channels_sum / num_batches

    # std = sqrt(E[X^2] - (E[X])^2)
    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5

    return mean, std


train_tensor = SignalDataset((X_train, y_train))

test_tensor = SignalDataset((X_test, y_test))

train_loader = data_utils.DataLoader(
    dataset=train_tensor, batch_size=128, shuffle=True)

train_mean, train_std = get_mean_and_std(train_loader)

train_tf = transforms.Compose([transforms.Normalize(tuple(train_mean.tolist()),
                                                    tuple(train_std.tolist()))])

train_tensor = SignalDataset((X_train, y_train), transform=train_tf)
train_loader = data_utils.DataLoader(
    dataset=train_tensor, batch_size=128, shuffle=True)

"""## Define network and training loop"""


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(6, 32, kernel_size=4, stride=1, padding=1)
        self.mp1 = nn.MaxPool2d(kernel_size=4, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=1)
        self.mp2 = nn.MaxPool2d(kernel_size=4, stride=2)
        self.fc1 = nn.Linear(2304, 256)
        self.dp1 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(256, 3)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp1(self.conv1(x)))
        x = F.relu(self.mp2(self.conv2(x)))
        x = x.view(in_size, -1)
        x = F.relu(self.fc1(x))
        x = self.dp1(x)
        x = self.fc2(x)

        return x


cnn = CNN().double()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)

num_epochs = 62

if TRAIN_MODE:
    total_step = len(train_loader)
    loss_list = []
    acc_list = []
    for epoch in range(num_epochs):
        for i, (signals, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            # Run the forward pass
            signals = signals
            labels = labels

            outputs = cnn(signals.double())

            loss = criterion(outputs, labels.long())

            loss_list.append(loss.item())

            # Backprop and perform Adam optimisation

            loss.backward()
            optimizer.step()
            # Track the accuracy
            total = labels.size(0)
            _, predicted = torch.max(F.log_softmax(outputs.data, dim=1), 1)
            correct = (predicted == labels.long()).sum().item()
            acc_list.append(correct / total)

            if (epoch+1) % 1 == 0 or epoch == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Train Accuracy: {:.2f}%'
                    .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),
                            (correct / total) * 100))

    # if you need to save
    torch.save(cnn, 'cnnTC3_fold3_45_62epochs_5.pth')

"""## Model Inference"""

if INFERENCE_MODE:
    class AddGaussianNoise(object):
        def __init__(self, mean=0., snr=-4, clip=None, method='torch'):
            self.method = method
            self.clip = clip
            self.snr = snr
            self.mean = mean
            self.std = 1/(10**(snr/10))

        def __call__(self, tensor):
            np.random.seed(0)
            if self.method == 'numpy':
                if self.clip:
                    noise = torch.from_numpy(
                        np.clip(np.random.normal(0, 1, (40, 40)), -self.clip, self.clip))
                else:
                    noise = torch.from_numpy(
                        np.random.normal(0, self.std, (40, 40)))
                return tensor + noise
            else:
                torch.manual_seed(0)
                return tensor + torch.randn(tensor.size()) * self.std + self.mean

        def __repr__(self):
            return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)


    # load model
    model = torch.load('cnnTC3_fold3_45_62epochs_5.pth')
    model.eval()

    test_tf = transforms.Compose([  # AddGaussianNoise(0, -4),
        transforms.Normalize(tuple(train_mean.tolist()),
                            tuple(train_std.tolist())),
        AddGaussianNoise(0, 8, method='torch'),
    ])

    test_tensor = SignalDataset((X_test, y_test), transform=test_tf)
    test_loader = data_utils.DataLoader(
        dataset=test_tensor, batch_size=1024, shuffle=False)

    test_noise_mean, test_noise_std = get_mean_and_std(test_loader)

    test_tf = transforms.Compose([  # AddGaussianNoise(0, -4),
        transforms.Normalize(tuple(train_mean.tolist()),
                            tuple(train_std.tolist())),
        AddGaussianNoise(0, 8, method='torch'),
        transforms.Normalize(tuple(test_noise_mean.tolist()),
                            tuple(test_noise_std.tolist())),
    ])

    test_tensor = SignalDataset((X_test, y_test), transform=test_tf)
    test_loader = data_utils.DataLoader(
        dataset=test_tensor, batch_size=1024, shuffle=False)

    epoch=62
    total_step = len(test_loader)
    print(total_step)
    loss_list_test = []
    acc_list_test = []
    with torch.no_grad():
        for i, (signals, labels) in enumerate(test_loader):
            # Run the forward pass
            signals = signals
            labels = labels
            outputs = model(signals.double())
            loss = criterion(outputs, labels.long())
            loss_list_test.append(loss.item())
            if epoch % 10 == 0:
                print(loss)
            total = labels.size(0)
            _, predicted = torch.max(F.log_softmax(outputs.data, dim=1), 1)
            correct = (predicted == labels.long()).sum().item()
            acc_list_test.append(correct / total)
            if (epoch) % 1 == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'
                    .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),
                            (correct / total) * 100))
    print('Total average accuracy: ' + str(np.mean(acc_list_test) * 100))
